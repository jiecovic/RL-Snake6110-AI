# configs/example.yaml
# Example configuration for training a PPO agent on Snake
# Training-only (headless). Human pygame play is handled by snake-play / watch scripts.

run:
  name: snake_ppo
  seed: 42
  num_envs: 16
  total_timesteps: 100_000_000
  checkpoint_freq: 1_000_000
  resume_checkpoint: null

level:
  height: 13
  width: 22
  food_count: 1

env:
  id: pov_pixel                 # env class selector (ENV_REGISTRY key)
  max_steps: 1_000              # episode truncation (RL wrapper concern)

# NOTE: observation.type is currently informational only (not used for dispatch).
# The actual env variant is selected via env.id, and the kwargs must match that env.
observation:
  type: egocentric
  params:
    view_radius: 10

model:
  features_extractor:
    type: combined
    cnn:
      type: tile4
      features_dim: 512
  net_arch: [256, 128]

ppo:
  n_steps: 2048
  batch_size: 128
  n_epochs: 2
  gamma: 0.999
  ent_coef: 0.03
  learning_rate: 3e-4
  verbose: 0
